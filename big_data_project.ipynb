Big Data Project
Synthesized version

import pandas as pd #makes data frames possible
import numpy as np #helps with data munging
import matplotlib.pyplot as plt #more powerful graphs
import os #aides in getting current directory
import scipy
import sklearn
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
#Import models from scikit learn module:
from sklearn.cross_validation import KFold   #For K-fold cross validation
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import classification_report
from sklearn import model_selection

#Geting the current working directory
os.getcwd()

#Dataset:
#http://koaning.io/fun-datasets.html
#many people write this as df= and not data=  
#df stands for the data frame that panda puts data into

#Reference:
#http://fairmodel.econ.yale.edu/rayfair/pdf/1978A200.PDF
data = pd.read_csv('/Users/sheilatupker/Desktop/BigDataProject/affairs.csv')

Getting to Know Data

data.head()#gives the first 5 records of the dataset
data.tail() #gives the last five records of the dataset

#Gives summary of numerical values 
#mostly categorical data so look at binary categories which will tell me more then some of the
#other attributes. This type of summary is always better for continious data however
#things can be garnered from it for discrete data (like my data).
data.describe() #the 5 number statistical summary

data.shape #quick and dirty dataset dimensions
#Gives the number of columns and rows, plus additional info
#601 rows and 9 columns
data.info 

Now testing out the some different ways to find Null values

data.isnull().head() #shows whether each value is null or not
#used head to keep nbr of records to five

data.isnull().values #shows same as above but in array so more succinct

data.isnull().any() #same as above but more succinct (by column)- perhaps my most preferred

#Finds the null values the most succinct way, returning one value
#No null values
data.isnull().values.any()

Distribution Analysis With A Visual Focus

#Boxplot of age variable
#A good way to see outliers and easily see and mark quartiles
data.boxplot(column='age')

#checking for outliers
#Finds max as we are looking for the outlier from above
#57 is not really an outlier so it will keep it in the data set
max(data['age'])

#Find unique values in the age var
#make sure all the data is good and included in the original experiment
data.age.unique()

max(data.age.unique()) #just checking another way - just to try out and test
#another method

#We decided on keeping age variable, even though it is an outlier it should be included in this dataset

#years married
data.ym.unique()

#I decided on keeping the 0.125 age variable
#even though it is an outlier it should be included in this dataset
#there is no reason not to include a newly married couple

A histogram divides the number line into equal intervals and shows how many values fall into each interval. A boxplot divides the data into four equal parts and shows what part of the number line is covered by each portion of the data. From a boxplot, you can see the five-number summary exactly and outliers are clearly marked. These must be estimated from a histogram and can be difficult to estimate. From a histogram, you can estimate the mean by estimating a balance point for the distribution. You cannot do this with a boxplot. A histogram will reveal the frequency of the data within an interval. You do not know the exact values, but you know how many are within the given boundaries. You know a lower and upper bound but not necessarily the exact least and greatest value. You know where there are clusters of data and where there are gaps. With a boxplot, you get a sense of the basic shape of the distribution, but you cannot see clusters or gaps

#Histogram of education variable
#it seems that most people have education
data['education'].hist(bins=50)

#Histogram of number of affairs variable
data['nbaffairs'].hist(bins=50)

#Histogram of occupation variable
data['occupation'].hist(bins=50)

#Histogram of rate of happiness variable
data['rate'].hist(bins=50)

#Histogram of relgiousness variable
data['religious'].hist(bins=50)

#Histogram of years married variable
data['ym'].hist(bins=50)

#Histogram of age variable
data['age'].hist(bins=50)

#The number of affairs
data.nbaffairs.unique()

#Creates column affair for yes or no / zero or one
#Added this var so I would know whether an affair was had or not
#irregardless of number of times. 
data['affair'] = 0

#append this newly created var to each record in the dataset
​
#Create a temp list
tempAffair = []

#For each row in the column,
for row in data['nbaffairs']:
    #if != 0 then a 1 gets assigned
    if row != 0:
        #append a 1
        tempAffair.append(1)
    #else its a 0
    else:
        tempAffair.append(0)
​
#Creates a column from the list
data['affair'] = tempAffair

#checking to see if newly appended var of affair is there and it is - yay!
data.head()

#checking the tail as well, just for good measure
data.tail()

data['religious'].value_counts()

data['education'].value_counts()

data['occupation'].value_counts()

data['age'].value_counts()

data['ym'].value_counts()

data['child'].value_counts()

data['nbaffairs'].value_counts()

#how many in each group per a categorical var
data['sex'].value_counts() 

#Shows how happy were they in their marriages
#5 is the highest and 1 is the lowest
data['rate'].value_counts() 

data['affair'].value_counts()
#For categorical data I have to check and see if there is a good balance
#for your target variable (affair var in this case) specifically.
#This imbalance is troubling as it could bias our results in Logistic Regression
#The overwhelming number of 0's will affect the LR estimation value of the 1's.
#So our results will be biased.
#Will have to change sampling before sending it into LR model (see SMOTE below)

#Now we know the data is munged - no nulls and the existing values are valid

#Now handle changing sex and child into categorical data
from sklearn.preprocessing import LabelEncoder
var_mod = ['sex','child']
le = LabelEncoder()
for i in var_mod:
    data[i] = le.fit_transform(data[i])
data.dtypes 

data.head()

#Views the impact of each variable and compares to them to one another
temp1 = data['sex'].value_counts(ascending=True)
temp2 = data.pivot_table(values='affair',index=['sex'],aggfunc=lambda x: x.map({1:1,0:0}).mean())
print ('Frequency Table for sex:')
print (temp1)

print ('\nProbility of having an affair based on sex:' )
print (temp2)
#Shows there is not a significant difference in a male or female in the probability of having an affair

#Views the impact of each variable and compares to them to one another
temp1 = data['child'].value_counts(ascending=True)
temp2 = data.pivot_table(values='affair',index=['child'],aggfunc=lambda x: x.map({1:1,0:0}).mean())
print ('Frequency Table for child:')
print (temp1)

print ('\nProbability of having an affair based on existence of child :' )
print (temp2)
#Shows there is a significant difference in having an affair based on whether or not one has a child

#View the impact of each variable and compares to them to one another
temp1 = data['religious'].value_counts(ascending=True)
temp2 = data.pivot_table(values='affair',index=['religious'])
print ('Frequency Table for religious:')
print (temp1)
​
print ('\nProbability of having an affair based on religious :' )
print (temp2)
#Shows there is a significant association in having an affair based on whether or not one very religious

#View the impact of each variable and compares to them to one another
temp1 = data['education'].value_counts(ascending=True)
temp2 = data.pivot_table(values='affair',index=['education'])
print ('Frequency Table for education:')
print (temp1)
​
print ('\nProbability of having an affair based on education :' )
print (temp2)
#Shows they are all in a similiar range but if one has a some graduate work 
#they would be least likely to have an affair in this group

#Views the impact of each variable and compares to them to one another
temp1 = data['age'].value_counts(ascending=True)
temp2 = data.pivot_table(values='affair',index=['age'])
print ('Frequency Table for age:')
print (temp1)
​
print ('\nProbability of having an affair based on age :' )
print (temp2)
#Shows there is a significant association in having an affair based on age.

#Views the impact of each variable and compares to them to one another
temp1 = data['occupation'].value_counts(ascending=True)
temp2 = data.pivot_table(values='affair',index=['occupation'])
print ('Frequency Table for occupation:')
print (temp1)
​
print ('\nProbability of having an affair based on occupation :' )
print (temp2)
#Shows they are fairly similar

#Views the impact of each variable and compares to them to one another
temp1 = data['rate'].value_counts(ascending=True)
temp2 = data.pivot_table(values='affair',index=['rate'])
print ('Frequency Table for rate:')
print (temp1)
​
print ('\nProbability of having an affair based on rate :' )
print (temp2)
#Shows there is a significant association with happiness on marriage 
#Very unhappy and unhappy are likley to have n affair

fig = plt.figure(figsize=(8,4))
temp2.plot(kind = 'bar')
plt.xlabel('Rate Happiness')
plt.ylabel('Probability of Affair')
plt.title("Probability of Having an Affair Based on Rate of Happiness")
plt.show()

Splitting Data For Training and Testing

#Before you split data do these things:
#1)Check Null Values (if null values exist - deal with them), 
#2)Separate x and y (tell the computer what my x - feature vars, y is your target var)
#3)Encoding (be careful of ordinal - they would need order vs nominal variables which don't have order
# you would use one hot encoding to make sure they don't get mixed with ordinal encoding)

#indexing to separate my data set into x and y
x = data.iloc[:, :-1].values
y = data.iloc[:, -1:].values.ravel()
​

#80/20 split  random state will keep your test   stratify will mimic the distr of y
x_train, x_test,y_train, y_test = train_test_split(x,y,test_size = .2,random_state=0, stratify=y)

#to handle the imbalance in 0s and 1s in the target var 'affair'
update_y = SMOTE(random_state=3, ratio = 1.0)
update_x_train, update_y_train = update_y.fit_sample(x_train, y_train)
​
#now the data is more balanced
print (np.bincount(update_y_train))
#now everything is checked to go into our models
#we do not need standardization for decision trees because it robust to numerical instabaility
#and it doesn't use any distance calculation

Modeling Function

#Classification:
#This is a classification problem.  We are trying to classify the data according to whether or not a instance had an affair.
​
#defining a function called classification_model that has the parameters- model, x_train, x_test, y_train, y_test
#model = is DecisionTreeClassifier(), RandomForrestClassifier(), LogisticRegression()
#data = (data = pd.read_csv('/Users/sheilatupker/Desktop/BigDataProject/affairs.csv'))
#predictors = ['sex','age','ym','child','religious','education','occupation','rate'] any single or combo of the group (x)
#outcome = 'affair' 0 no affair  or 1 for affair (y)

def classification_model(model, x_train, x_test, y_train, y_test)
	#This is the learning stage of the algorithm - it is called Fitting the model.
	#As parameters it takes in training preditors and outome.
	model.fit(x_train, y_train)

	#Make predictions on testing set: 
	predictions = model.predict(x_test)

	#dot format so put whatever the accuracy is inside of the curly braces.
	#then apply 3 decimal places
	#then sub this in for %s and voila
	print('Train Accuracy : {0:.3%}'.format(metrics.accuracy_score(y_train, model.predict(x_train))))
	print('Test Accuracy: {0:.3%}'.format(metrics.accuracy_score(y_test, predictions)))

	# Calculate precision, recall, and fscore
	precision, recall, fscore, support = precision_recall_fscore_support(y_test,predictions, average='micro')
	print('Fscore:', fscore)

	# Classification report
	print(classification_report(y_test, predictions))

	#cross validation to check for over fitting
	#creating a variable, kf, for the constructor of the built in object KFOLD()
	kf = model_selection.KFold(n_splits=6, random_state = 0)
	results = model_selection.cross_val_score(model, x_train, y_train, cv=kf, scoring=scoring)
	print("6-fold cross validation average accuracy: %0.2f (+/- %0.2f)" % (results.mean(), results.std() * 2))

Decision Tree

#This provides a decision tree boundary for how the data is separated.
#example: categorizing animal species - does it produce milk or not
#if it is not producing milk then stop bc its something other than mammal
#if it produces milk then keep going because it is a mammal
#next you might ask does it have feathers
#the end result will show on all of your edges should be each of of your variables
#you should be able to classify something based on how it performs in the decision tree
#for example - are you happy in your relationship?  How many years married?
#so its going to learn the most important variables and put them in the decision ranked by importance
#and let you know this is likely for an affair - yes.

model = DecisionTreeClassifier()
classification_model(model, update_x_train, x_test, update_y_train, y_test)

#accuracy - is how many rows in my data fields (in the training set) produced the correct results by going through this decision tree.
#Cross-validation - say you have a data table of the data we have collected.  One way to validate is to say you will take 10 percent and
#remove that from your training model. so training is 90 percent and testing is 10 percent.
#you train your model on the training part (90 percent).
#this will get an accuracy of ex. 98.669 percent.
#but then take the test set and run it through the same model and see what the accuracy score is now
#that is the 59.708(the cross validation score).
#Too high of an accuracy percentage is a sign of overfitting. Then adjustments will need to be made.
#The cross validation was done on 5 fold so it ran five times and this is an average of those runs.
​
#We noticed there is a randomness in our outputs (varied group member to group member as well as when one group member would 
#rerun the cross validation score changes  - look into where and why this is happening besides the different 
#data sets being chosen.)

#Overfitting has occured.
#model.tree lets you look at the var inside your decision tree classifier that defines my entire tree
#node_count tells me how many nodes are in my tree
model.tree_.node_count

#I would like a higher cross-validation score .
#I feel that 98.669 is so much higher than 59.708 which leads me to beleive there is possibly over-fitting.
#I can tweak - criterion parameters
#maxLeaveNodes - 100
model = DecisionTreeClassifier(max_leaf_nodes = 100)
classification_model(model, update_x_train, x_test, update_y_train, y_test)

#I would like a higher cross-validation score .
#I feel that 91.348 is so much higher than 61.212 which leads me to beleive there is possibly over-fitting.
#I can tweak - criterion parameters
#maxLeaveNodes - 10
model = DecisionTreeClassifier(max_leaf_nodes = 10)
classification_model(model, update_x_train, x_test, update_y_train, y_test)

#more tweaking - max_depth = 3
model = DecisionTreeClassifier(max_leaf_nodes = 10, max_depth = 3)
classification_model(model, update_x_train, x_test, update_y_train, y_test)

#find out how each variable weighed in
model.feature_importances_

#more tweaking - taking out sex and ym
#predictor vars - everything but affair and nbraffair
#model same as above
sub_update_x_train = update_x_train[:,[1,3,4,5,7]]
sub_x_test = x_test[:,[1,3,4,5,7]]
classification_model(model, sub_update_x_train, sub_x_test, update_y_train, y_test)

#more tweaking - try top three vars only
#model same as above
sub_update_x_train = update_x_train[:,[1,4,7]]
sub_x_test = x_test[:,[1,4,7]]
classification_model(model, sub_update_x_train, sub_x_test, update_y_train, y_test)

#more tweaking - try top three vars only and not specify max_depth
model = DecisionTreeClassifier(max_leaf_nodes = 10)
#same variables as before
classification_model(model, sub_update_x_train, sub_x_test, update_y_train, y_test)

Random Forest

#Random forest takes a number of decision trees and averages them together
#so it is more accurate of the overall picture than a decision tree

rc.classifier = RandomForestClassifier(random_state=0)

Feature Importance and Selection

#train the model - train on my training sets that have been over sampled
rc_classifier.fit(update_x_train, update_y_train)

#find out how which features are more important to the model
rc_rank = rc_classifier.feature_importances_

#place this into an array
rc_array = pd.Series(rc_rank, data.columns[:-1])
#sorting it in Desc and inplace resaves it as feature_array
rc_array.sort_values(ascending = False, inplace = True)
print (rc_array)

#Feature selection - evaluate most significant features for random forest 
#classifier - this takes the topic one feature and runs the random forest 
#and calculate an fscore, repeate this process with the n-1 feature.  This tells
#me how many features will give you the best fscore.
#using random forest for feature selection
rf_importances = []
fscores = []
#enter columns by order of importance
f_cols = [7,4,5,1,6,2,3,0]
#going to do this 10 times by each time creating 25 different trees
for k in range(1,10):
    # declare model
    rfc = RandomForestClassifier(random_state=0, n_estimators=25)
    
    # train model
    rfc.fit(update_x_train[:,f_cols[0:k]], update_y_train)
    
    # get feature importances
    importances = rfc.feature_importances_.tolist()
​
    # test model - 
    y_pred = rfc.predict(x_test[:,f_cols[0:k]])
 
    # Get precision, recall, fscore, and support
    precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='micro')
    rf_importances.append(importances)
    fscores.append(fscore)
​
print('Fscores:', fscores)
#fscores - measure of accuracy
#the top two have the same importance, the top four have the highest f score which means to work with
#the top four variables which are rate, religious, education, and age

#model for random forest
#first we will use all variables
#running many decision trees and averages them
model = RandomForestClassifier(random_state=0)
classification_model(model, update_x_train, x_test, update_y_train, y_test)

#a subset of the data taken from our feature importances
#using the top 4 attributes we learned from above
sub_update_x_train = update_x_train[:,[1,4,5,7]]
sub_x_test = x_test[:,[1,4,5,7]]

#model for random forest
#second will use only the top 4 vars 
classification_model(model, sub_update_x_train, sub_x_test, update_y_train, y_test)

Logistic Regression

#logistic regression with all variables
#just a different model just to see the differnces or sameness for accuracy

plot of data on x and y with data points
i want to find a statistcal model that describes the relationship between x and y
so they do some math and come up with an equation for a line y = mx+b they put some
values in for m and b 
then they see how good a fit it is and if it is not a good they look at the errors
and make a new line y = m1xb1 witha new line - is it a better fit?
It will keep iterating until it gets to the smallest possible error
​
The general idea of regression is you have some equation (some math) wher eyou try to do 
the prediction and you try keep errors down and keep going until you get a min error
​
Logistic Regression is using the logistic equation instead of the linear equation

#penalty - keeps from overfitting
#c - helps balance out data
#random_sate for data consistency
#initiated model
model = LogisticRegression(penalty='l2', C=1.0, random_state=0)
classification_model(model, update_x_train, x_test, update_y_train, y_test)

#logistic regression with top four
#same model as before
classification_model(model, sub_update_x_train, sub_x_test, update_y_train, y_test)

#what we learned:
#kfold cross validation is a better method to use for logistic regression.
#compare decision tree with random forest with logistic regression and see which one
#is better and why.
